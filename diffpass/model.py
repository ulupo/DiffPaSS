# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/model.ipynb.

# %% auto 0
__all__ = ['IndexPair', 'IndexPairsInGroup', 'IndexPairsInGroups', 'GeneralizedPermutation', 'MatrixApply',
           'PermutationConjugate', 'global_argmax_from_group_argmaxes', 'apply_hard_permutation_batch_to_similarity',
           'TwoBodyEntropyLoss', 'MILoss', 'HammingSimilarities', 'Blosum62Similarities', 'BestHits',
           'InterGroupSimilarityLoss', 'IntraGroupSimilarityLoss']

# %% ../nbs/model.ipynb 4
# Stdlib imports
from collections.abc import Iterable, Sequence
from typing import Optional, Union, Literal
from warnings import warn
from functools import partial

# NumPy
import numpy as np

# PyTorch
import torch
from torch.nn import Module, ParameterList, Parameter

# DiffPaSS imports
from .gumbel_sinkhorn_ops import gumbel_sinkhorn, gumbel_matching
from diffpass.entropy_ops import (
    smooth_mean_one_body_entropy,
    smooth_mean_two_body_entropy,
)
from .constants import get_blosum62_data
from diffpass.sequence_similarity_ops import (
    smooth_hamming_similarities_dot,
    smooth_hamming_similarities_cdist,
    smooth_substitution_matrix_similarities_dot,
    smooth_substitution_matrix_similarities_cdist,
    soft_best_hits,
    hard_best_hits,
)

# Type aliases
IndexPair = tuple[int, int]  # Pair of indices
IndexPairsInGroup = list[IndexPair]  # Pairs of indices in a group of sequences
IndexPairsInGroups = list[IndexPairsInGroup]  # Pairs of indices in groups of sequences

# %% ../nbs/model.ipynb 7
def _consecutive_slices_from_sizes(group_sizes: Optional[Sequence[int]]) -> list[slice]:
    if group_sizes is None:
        return [slice(None)]
    cumsum = np.cumsum(group_sizes).tolist()

    return [slice(start, end) for start, end in zip([0] + cumsum, cumsum)]

# %% ../nbs/model.ipynb 9
class GeneralizedPermutation(Module):
    """Generalized permutation layer implementing both soft and hard permutations."""

    number_effective_fixed_pairings_: torch.Tensor
    number_effective_nonfixed_pairings_: torch.Tensor
    total_number_effective_fixed_pairings_: torch.Tensor

    def __init__(
        self,
        *,
        group_sizes: Sequence[int],
        batch_size: Optional[int] = None,
        fixed_pairings: Optional[
            Union[IndexPairsInGroups, list[IndexPairsInGroups]]
        ] = None,
        tau: float = 1.0,
        n_iter: int = 1,
        noise: bool = False,
        noise_factor: float = 1.0,
        noise_std: bool = False,
        unbias_lsa: bool = True,  # Unbias the linear sum assignment (LSA) problem for hard permutations
        mode: Literal["soft", "hard"] = "soft",
    ) -> None:
        super().__init__()
        self.group_sizes = group_sizes
        ### Buffer initialization ###
        self.register_buffer(
            "group_sizes_", torch.tensor(self.group_sizes, dtype=torch.long)
        )
        [
            setattr(
                self,
                f"effective_fixed_pairings_{group_idx}_",
                torch.empty((0, 2), dtype=torch.long),
            )
            for group_idx, s in enumerate(self.group_sizes)
        ]
        self.register_buffer(
            "number_effective_fixed_pairings_",
            torch.zeros(len(self.group_sizes), dtype=torch.long),
        )
        self.register_buffer("number_effective_nonfixed_pairings_", self.group_sizes_)
        self.register_buffer(
            "total_number_effective_fixed_pairings_",
            torch.tensor(0, dtype=torch.long),
        )
        ### End buffer initialization ###

        self.init_batch_size_fixed_pairings_and_log_alphas(batch_size, fixed_pairings)

        self.tau = tau
        self.n_iter = n_iter
        self.noise = noise
        self.noise_factor = noise_factor
        self.noise_std = noise_std
        self.unbias_lsa = unbias_lsa
        self.mode = mode

    def init_batch_size_fixed_pairings_and_log_alphas(
        self,
        batch_size: Optional[int],
        fixed_pairings: Optional[Union[IndexPairsInGroups, list[IndexPairsInGroups]]],
        device: Optional[torch.device] = None,
    ) -> None:
        """Initialize fixed pairings and parameterization matrices."""
        self.to(device=device)
        self._validate_batch_size_and_fixed_pairings(
            batch_size, fixed_pairings, device=device
        )
        # Note: Instance attrs below can't be set in __init__ for bootstrap to work
        self.batch_size = batch_size
        self.fixed_pairings = fixed_pairings

        # Initialize parameterization matrices ('log-alphas')
        # By default, initialize all parametrization matrices to zero
        if self.batch_size is None:
            self.log_alphas = ParameterList(
                [
                    Parameter(torch.zeros(s, s, device=device), requires_grad=bool(s))
                    for s in self.number_effective_nonfixed_pairings_
                ]
            )
        else:
            self.log_alphas = ParameterList(
                [
                    Parameter(
                        torch.zeros(self.batch_size, s, s, device=device),
                        requires_grad=True,
                    )
                    for s in self.group_sizes
                ]
            )

    def _validate_batch_size_and_fixed_pairings(
        self,
        batch_size: Optional[int],
        fixed_pairings: Optional[Union[IndexPairsInGroups, list[IndexPairsInGroups]]],
        device: Optional[torch.device] = None,
    ) -> None:
        if not fixed_pairings:
            # None or [] for fixed_pairings can only be if batch_size is None, as per initialization
            return

        if batch_size is None:
            # TODO guard this check against case group_sizes == batch_size
            if len(fixed_pairings) != len(self.group_sizes):
                raise ValueError(
                    "If `fixed_pairings` is provided, it must have the same length as "
                    "`group_sizes`."
                )
            else:
                number_effective_fixed_pairings = torch.tensor(
                    [len(fp) for fp in fixed_pairings],
                    device=device,
                    dtype=torch.long,
                )
                self.number_effective_fixed_pairings_ = number_effective_fixed_pairings
                self.number_effective_nonfixed_pairings_ = (
                    self.group_sizes_ - number_effective_fixed_pairings
                )
                for group_idx, s in enumerate(self.group_sizes):
                    not_fixed_mask_this_group = torch.ones(
                        s, s, dtype=torch.bool, device=device
                    )
                    effective_fixed_pairings_this_group = torch.tensor(
                        fixed_pairings[group_idx], device=device, dtype=torch.long
                    ).reshape(-1, 2)
                    num_nonfixed = self.number_effective_nonfixed_pairings_[group_idx]
                    if not num_nonfixed:
                        not_fixed_mask_this_group[...] = False
                    else:
                        not_fixed_mask_this_group[
                            effective_fixed_pairings_this_group[:, -1], :
                        ] = False
                        not_fixed_mask_this_group[
                            :, effective_fixed_pairings_this_group[:, -2]
                        ] = False
                        if num_nonfixed == 1:
                            # Note: Transpose because we must reverse the order of the pair
                            new_fp = not_fixed_mask_this_group.T.nonzero()
                            effective_fixed_pairings_this_group = torch.cat(
                                (effective_fixed_pairings_this_group, new_fp), dim=0
                            )
                            self.number_effective_fixed_pairings_[group_idx] += 1
                            self.number_effective_nonfixed_pairings_[group_idx] -= 1
                            not_fixed_mask_this_group[
                                new_fp[0][-1],
                                new_fp[0][-2],
                            ] = False
                    setattr(
                        self, f"_not_fixed_mask_{group_idx}", not_fixed_mask_this_group
                    )
                    setattr(
                        self,
                        f"effective_fixed_pairings_{group_idx}_",
                        effective_fixed_pairings_this_group,
                    )
                self.total_number_effective_fixed_pairings_ = torch.sum(
                    self.number_effective_fixed_pairings_
                )
        elif not isinstance(fixed_pairings, list) or len(fixed_pairings) != batch_size:
            # If batch_size is not None then fixed_pairings must be a list of length batch_size
            raise ValueError(
                "When `batch_size` is not None, `fixed_pairings` must be a list with "
                "length equal to `batch_size`. Each element in `fixed_pairings` must "
                "itself be a list of the same length as `group_sizes`."
            )
        else:
            # TODO check that each element in fixed_pairings is a list of lists of pairs of the correct lengths
            number_effective_fixed_pairings = torch.tensor(
                [
                    [len(fp) for fp in fixed_pairings_this_batch]
                    for fixed_pairings_this_batch in fixed_pairings
                ],
                device=device,
                dtype=torch.long,
            )
            self.number_effective_fixed_pairings_ = number_effective_fixed_pairings
            self.number_effective_nonfixed_pairings_ = (
                self.group_sizes_ - number_effective_fixed_pairings
            )
            for group_idx, s in enumerate(self.group_sizes):
                not_fixed_mask_this_group = torch.ones(
                    batch_size, s, s, dtype=torch.bool, device=device
                )
                # FIXME list comprehension is performance bottleneck
                effective_fixed_pairings_this_group = torch.tensor(
                    [
                        [batch_idx, pair[0], pair[1]]
                        for batch_idx, fixed_pairings_this_batch in enumerate(
                            fixed_pairings
                        )
                        if fixed_pairings_this_batch[group_idx]
                        for pair in fixed_pairings_this_batch[group_idx]
                    ],
                    device=device,
                    dtype=torch.long,
                ).reshape(
                    -1, 3
                )  # Column 0 is batch index
                # First fill in the fixed rows/columns as given by fixed_pairings
                not_fixed_mask_this_group[
                    effective_fixed_pairings_this_group[:, 0],
                    effective_fixed_pairings_this_group[:, -1],
                    :,
                ] = False
                not_fixed_mask_this_group[
                    effective_fixed_pairings_this_group[:, 0],
                    :,
                    effective_fixed_pairings_this_group[:, -2],
                ] = False
                # Now fill in the extra batches that are effectively fully fixed
                extra_effectively_fixed_batch_idxs = (
                    self.number_effective_nonfixed_pairings_[:, group_idx] == 1
                ).nonzero(as_tuple=True)[0]
                if extra_effectively_fixed_batch_idxs.size(0):
                    extra_effective_fixed_pairings_this_group = torch.hstack(
                        (
                            extra_effectively_fixed_batch_idxs.unsqueeze(-1),
                            torch.cat(
                                [
                                    # Note: Transpose because we must reverse the order of the pair
                                    not_fixed_mask_this_group[
                                        batch_idx, ...
                                    ].T.nonzero()
                                    for batch_idx in extra_effectively_fixed_batch_idxs
                                ],
                                dim=0,
                            ),
                        )
                    )
                    effective_fixed_pairings_this_group = torch.cat(
                        (
                            effective_fixed_pairings_this_group,
                            extra_effective_fixed_pairings_this_group,
                        ),
                        dim=0,
                    )
                    not_fixed_mask_this_group[
                        extra_effective_fixed_pairings_this_group[:, 0],
                        extra_effective_fixed_pairings_this_group[:, -1],
                        extra_effective_fixed_pairings_this_group[:, -2],
                    ] = False
                    self.number_effective_fixed_pairings_[
                        extra_effectively_fixed_batch_idxs, group_idx
                    ] += 1
                    self.number_effective_nonfixed_pairings_[
                        extra_effectively_fixed_batch_idxs, group_idx
                    ] -= 1
                setattr(self, f"_not_fixed_mask_{group_idx}", not_fixed_mask_this_group)
                setattr(
                    self,
                    f"effective_fixed_pairings_{group_idx}_",
                    effective_fixed_pairings_this_group,
                )

            self.total_number_effective_fixed_pairings_ = (
                self.number_effective_fixed_pairings_.sum(-1)
            )

    def _not_fixed_mask(self, group_idx: int) -> torch.Tensor:
        return getattr(self, f"_not_fixed_mask_{group_idx}")

    def effective_fixed_pairings_(self, group_idx: int) -> torch.Tensor:
        return getattr(self, f"effective_fixed_pairings_{group_idx}_")

    @property
    def mode(self) -> str:
        return self._mode

    @mode.setter
    def mode(self, value) -> None:
        value = value.lower()
        if value not in ["soft", "hard"]:
            raise ValueError("mode must be either 'soft' or 'hard'.")
        self._mode = value.lower()
        _mat_fn_no_fixed = getattr(self, f"_{self._mode}_mat")
        if self.batch_size is None:
            self._fwd_one_group = (
                (lambda group_idx: _mat_fn_no_fixed(self.log_alphas[group_idx]))
                if not self.fixed_pairings
                else self._impl_fixed_pairings(_mat_fn_no_fixed)
            )
        else:
            self._fwd_one_group = self._impl_fixed_pairings_batched(_mat_fn_no_fixed)

    def soft_(self) -> None:
        self.mode = "soft"

    def hard_(self) -> None:
        self.mode = "hard"

    def _impl_fixed_pairings(self, func: callable) -> callable:
        """Include fixed pairings in the Gumbel-Sinkhorn or Gumbel-matching operators."""

        def wrapper(group_idx: int) -> torch.Tensor:
            s = self.group_sizes[group_idx]
            mat = func(self.log_alphas[group_idx])
            row_group, col_group = self.effective_fixed_pairings_(group_idx).T
            not_fixed_mask = self._not_fixed_mask(group_idx)  # (s, s)
            mat_all = torch.zeros(
                s,
                s,
                dtype=mat.dtype,
                layout=mat.layout,
                device=mat.device,
            )
            # mat_all[j, i] = 1 means that row i becomes row j under a permutation,
            # using our conventions
            mat_all[col_group, row_group] = 1
            mat_all.masked_scatter_(not_fixed_mask, mat)

            return mat_all

        return wrapper

    def _impl_fixed_pairings_batched(self, func: callable) -> callable:
        """Include fixed pairings in the Gumbel-Sinkhorn or Gumbel-matching operators."""
        dtype = self.log_alphas[0][0].dtype
        layout = self.log_alphas[0][0].layout
        device = self.log_alphas[0][0].device

        def wrapper(group_idx: int) -> torch.Tensor:
            s = self.group_sizes[group_idx]
            effective_fixed_pairings_this_group = self.effective_fixed_pairings_(
                group_idx
            )
            not_fixed_masks = self._not_fixed_mask(group_idx)  # (batch_size, s, s)
            mats_all = torch.full(
                (self.batch_size, s, s),
                -torch.inf,
                dtype=dtype,
                layout=layout,
                device=device,
            )
            # mat_all[j, i] = 1 means that row i becomes row j under a permutation,
            # using our conventions
            mats_all[
                effective_fixed_pairings_this_group[:, 0],
                effective_fixed_pairings_this_group[:, -1],
                effective_fixed_pairings_this_group[:, -2],
            ] = 0.0
            mats_all.masked_scatter_(
                not_fixed_masks, self.log_alphas[group_idx][not_fixed_masks]
            )

            return func(mats_all)

        return wrapper

    def _soft_mat(self, log_alpha: torch.Tensor) -> torch.Tensor:
        """Evaluate the Gumbel-Sinkhorn operator on `log_alpha`."""
        return gumbel_sinkhorn(
            log_alpha,
            tau=self.tau,
            n_iter=self.n_iter,
            noise=self.noise,
            noise_factor=self.noise_factor,
            noise_std=self.noise_std,
        )

    def _hard_mat(self, log_alpha: torch.Tensor) -> torch.Tensor:
        """Evaluate the Gumbel-matching operator on `log_alpha`."""
        return gumbel_matching(
            log_alpha,
            noise=self.noise,
            noise_factor=self.noise_factor,
            noise_std=self.noise_std,
            unbias_lsa=self.unbias_lsa,
        )

    def forward(self) -> list[torch.Tensor]:
        """Compute the soft/hard permutations, according to the current mode."""

        return [
            self._fwd_one_group(group_idx) for group_idx in range(len(self.group_sizes))
        ]


class MatrixApply(Module):
    """Apply matrices to chunks of a tensor of shape (n_samples, length, alphabet_size)
    and collate the results."""

    def __init__(self, group_sizes: Sequence[int]) -> None:
        super().__init__()
        self.group_sizes = group_sizes
        self._group_slices = _consecutive_slices_from_sizes(self.group_sizes)

    def forward(self, x: torch.Tensor, *, mats: Sequence[torch.Tensor]) -> torch.Tensor:
        ensemble_shape = mats[0].shape[:-2]
        out = torch.full_like(x, torch.nan).repeat(*ensemble_shape, 1, 1, 1)
        for mats_this_group, sl in zip(mats, self._group_slices):
            out[..., sl, :, :].copy_(
                torch.tensordot(mats_this_group, x[sl, :, :], dims=1)
            )

        return out


class PermutationConjugate(Module):
    """Conjugate blocks of a square 2D tensor of shape (n_samples, n_samples) by
    permutation matrices."""

    def __init__(self, group_sizes: Sequence[int]) -> None:
        super().__init__()
        self.group_sizes = group_sizes
        self._group_slices = _consecutive_slices_from_sizes(self.group_sizes)

    def forward(self, x: torch.Tensor, *, mats: Sequence[torch.Tensor]) -> torch.Tensor:
        ensemble_shape = mats[0].shape[:-2]
        out1 = torch.full_like(x, torch.nan).repeat(*ensemble_shape, 1, 1)
        out2 = torch.full_like(x, torch.nan).repeat(*ensemble_shape, 1, 1)
        # (P * A) * P.T
        for mats_this_group, sl in zip(mats, self._group_slices):
            out1[..., sl, :].copy_(torch.tensordot(mats_this_group, x[sl, :], dims=1))
        for mats_this_group, sl in zip(mats, self._group_slices):
            out2[..., :, sl].copy_(
                torch.einsum("...ij,...kj->...ik", out1[..., :, sl], mats_this_group)
            )

        return out2


def global_argmax_from_group_argmaxes(mats: Iterable[torch.Tensor]) -> torch.Tensor:
    global_argmax = []
    start_idx = 0
    for mats_this_group in mats:
        global_argmax.append(mats_this_group.argmax(-1) + start_idx)
        start_idx += mats_this_group.shape[-1]

    return torch.cat(global_argmax, dim=-1)


def apply_hard_permutation_batch_to_similarity(
    *, x: torch.Tensor, perms: list[torch.Tensor]
) -> torch.Tensor:
    """
    Conjugate a single square matrix by a batch of hard permutations.

    Args:
        perms: List of batches of permutation matrices of shape (..., D, D).
        x: Matrix of shape (D, D).

    Returns:
        Batch of conjugated matrices of shape (..., D, D).
    """
    global_argmax = global_argmax_from_group_argmaxes(perms)
    x_permuted_rows = x[global_argmax]

    # Permuting columns is more involved
    index = global_argmax.view(*global_argmax.shape[:-1], 1, -1).expand(
        *global_argmax.shape, global_argmax.shape[-1]
    )
    # Example of gather with 4D tensor and dim=-1:
    # out[i][j][k][l] = input[i][j][k][index[i][j][k][l]]

    return torch.gather(x_permuted_rows, -1, index)

# %% ../nbs/model.ipynb 13
class TwoBodyEntropyLoss(Module):
    """Differentiable extension of the mean of estimated two-body entropies between
    all pairs of columns from two one-hot encoded tensors."""

    def __init__(self):
        super().__init__()

    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        return smooth_mean_two_body_entropy(x, y)


class MILoss(Module):
    """Differentiable extension of minus the mean of estimated mutual informations
    between all pairs of columns from two one-hot encoded tensors."""

    def __init__(self):
        super().__init__()

    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        return smooth_mean_two_body_entropy(x, y) - smooth_mean_one_body_entropy(x)

# %% ../nbs/model.ipynb 18
class HammingSimilarities(Module):
    """Compute Hamming similarities between sequences using differentiable
    operations.

    Optionally, if the sequences are arranged in groups, the computation of
    similarities can be restricted to within groups.
    Differentiable operations are used to compute the similarities, which can be
    either dot products or an L^p distance function."""

    def __init__(
        self,
        *,
        group_sizes: Optional[Sequence[int]] = None,
        use_dot: bool = True,
        p: Optional[float] = None,
    ) -> None:
        super().__init__()
        self.group_sizes = group_sizes
        self.use_dot = use_dot
        self.p = p

        if self.use_dot:
            if self.p is not None:
                warn("Since a `p` was provided, `use_dot` will be ignored.")
            self._similarities_fn = smooth_hamming_similarities_dot
            self._similarities_fn_kwargs = {}
        else:
            if self.p is None:
                raise ValueError("If `use_dot` is False, `p` must be provided.")
            self._similarities_fn = smooth_hamming_similarities_cdist
            self._similarities_fn_kwargs = {"p": self.p}

        self._group_slices = _consecutive_slices_from_sizes(self.group_sizes)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        size = x.shape[:-3] + (x.shape[-3],) * 2
        out = torch.full(
            size, torch.nan, dtype=x.dtype, layout=x.layout, device=x.device
        )
        for sl in self._group_slices:
            out[..., sl, sl].copy_(
                self._similarities_fn(x[..., sl, :, :], **self._similarities_fn_kwargs)
            )

        return out


class Blosum62Similarities(Module):
    """Compute Blosum62-based similarities between sequences using differentiable
    operations.

    Optionally, if the sequences are arranged in groups, the computation of
    similarities can be restricted to within groups.
    Differentiable operations are used to compute the similarities, which can be
    either dot products or an L^p distance function."""

    def __init__(
        self,
        *,
        group_sizes: Optional[Sequence[int]] = None,
        use_dot: bool = True,
        p: Optional[float] = None,
        use_scoredist: bool = False,
        aa_to_int: Optional[dict[str, int]] = None,
        gaps_as_stars: bool = True,
    ) -> None:
        super().__init__()
        self.group_sizes = group_sizes
        self.use_dot = use_dot
        self.p = p
        self.use_scoredist = use_scoredist
        self.aa_to_int = aa_to_int
        self.gaps_as_stars = gaps_as_stars

        blosum62_data = get_blosum62_data(
            aa_to_int=self.aa_to_int, gaps_as_stars=self.gaps_as_stars
        )
        self.register_buffer("subs_mat", blosum62_data.mat)
        self.expected_value = blosum62_data.expected_value

        self._similarities_fn_kwargs = {"subs_mat": self.subs_mat}
        if self.use_dot:
            if self.p is not None:
                warn("Since a `p` was provided, `use_dot` will be ignored.")
            self._similarities_fn = smooth_substitution_matrix_similarities_dot
            self._similarities_fn_kwargs = {
                "use_scoredist": self.use_scoredist,
                "expected_value": self.expected_value,
            }
        else:
            if self.p is None:
                raise ValueError("If `use_dot` is False, `p` must be provided.")
            self._similarities_fn = smooth_substitution_matrix_similarities_cdist
            self._similarities_fn_kwargs = {"p": self.p}

        self._group_slices = _consecutive_slices_from_sizes(self.group_sizes)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        size = x.shape[:-3] + (x.shape[-3],) * 2
        out = torch.full(
            size, torch.nan, dtype=x.dtype, layout=x.layout, device=x.device
        )
        for sl in self._group_slices:
            out[..., sl, sl].copy_(
                self._similarities_fn(
                    x[..., sl, :, :],
                    subs_mat=self.subs_mat,
                    **self._similarities_fn_kwargs,
                )
            )

        return out

# %% ../nbs/model.ipynb 23
class BestHits(Module):
    """Compute (reciprocal) best hits within and between groups of sequences,
    starting from a similarity matrix.

    Best hits can be either 'hard', in which cases they are computed using the
    argmax, or 'soft', in which case they are computed using the softmax with a
    temperature parameter `tau`. In both cases, the main diagonal in the similarity
    matrix is excluded by setting its entries to minus infinity."""

    def __init__(
        self,
        *,
        reciprocal: bool = True,
        group_sizes: Optional[Sequence[int]],
        tau: float = 0.1,
        mode: Literal["soft", "hard"] = "soft",
    ) -> None:
        super().__init__()
        self.reciprocal = reciprocal
        self.group_sizes = group_sizes
        self._group_slices = _consecutive_slices_from_sizes(self.group_sizes)
        self.tau = tau
        self.mode = mode

    @property
    def mode(self) -> str:
        return self._mode

    @mode.setter
    def mode(self, value) -> None:
        value = value.lower()
        if value not in ["soft", "hard"]:
            raise ValueError("`mode` must be either 'soft' or 'hard'.")
        self._mode = value.lower()
        self._bh_fn = getattr(self, f"_{self._mode}_bh_fn")

    def soft_(self) -> None:
        self.mode = "soft"

    def hard_(self) -> None:
        self.mode = "hard"

    def _soft_bh_fn(self, similarities: torch.Tensor) -> torch.Tensor:
        """Compute soft best hits."""
        return soft_best_hits(
            similarities,
            reciprocal=self.reciprocal,
            group_slices=self._group_slices,
            tau=self.tau,
        )

    def _hard_bh_fn(self, similarities: torch.Tensor) -> torch.Tensor:
        """Compute hard best hits."""
        return hard_best_hits(
            similarities,
            reciprocal=self.reciprocal,
            group_slices=self._group_slices,
        )

    def forward(self, similarities: torch.Tensor) -> torch.Tensor:
        return self._bh_fn(similarities)

# %% ../nbs/model.ipynb 26
class InterGroupSimilarityLoss(Module):
    """Compute a loss that compares similarity matrices restricted to inter-group
    relationships.

    Similarity matrices are expected to be square and symmetric. The loss is computed
    by comparing the (flattened and concatenated) blocks containing inter-group
    similarities."""

    def __init__(
        self,
        *,
        # Number of entries in each group (e.g. species). Groups are assumed to be
        # contiguous in the input similarity matrices
        group_sizes: Sequence[int],
        # If not ``None``, custom callable to compute the differentiable score between
        # the flattened and concatenated inter-group blocks of the similarity matrices.
        # Default: dot product
        score_fn: Union[callable, None] = None,
    ) -> None:
        super().__init__()
        self.group_sizes = group_sizes
        self.score_fn = (
            partial(torch.tensordot, dims=1) if score_fn is None else score_fn
        )

        diag_blocks_mask = torch.block_diag(
            *[torch.ones((s, s), dtype=torch.bool) for s in self.group_sizes]
        )
        self.register_buffer(
            "_upper_no_diag_blocks_mask", torch.triu(~diag_blocks_mask)
        )

    def forward(
        self,
        similarities_x: torch.Tensor,
        similarities_y: torch.Tensor,
        *,
        mats: Optional[Sequence[torch.Tensor]] = None,
    ) -> torch.Tensor:
        # Input validation
        assert similarities_x.ndim >= 2 and similarities_y.ndim >= 2

        scores = self.score_fn(
            similarities_x[..., self._upper_no_diag_blocks_mask],
            similarities_y[..., self._upper_no_diag_blocks_mask],
        )
        loss = -scores

        return loss


class IntraGroupSimilarityLoss(Module):
    """Compute a loss that compares similarity matrices restricted to intra-group
    relationships.

    Similarity matrices are expected to be square and symmetric. Their diagonal
    elements are ignored if `exclude_diagonal` is set to ``True``.
    If `group_sizes` is provided, the loss is computed by comparing the flattened
    and concatenated upper triangular blocks containing intra-group similarities.
    Otherwise, the loss is computed by comparing the upper triangular part of the
    full similarity matrices."""

    def __init__(
        self,
        *,
        # Number of entries in each group (e.g. species). Groups are assumed to be
        # contiguous in the input similarity matrices
        group_sizes: Optional[Sequence[int]] = None,
        # If not ``None``, custom callable to compute the differentiable score between
        # the flattened and concatenated intra-group blocks of the similarity matrices
        # Default: dot product
        score_fn: Union[callable, None] = None,
        # If ``True``, exclude the diagonal elements from the computation
        exclude_diagonal: bool = True,
    ) -> None:
        super().__init__()
        self.group_sizes = group_sizes
        self.score_fn = (
            partial(torch.tensordot, dims=1) if score_fn is None else score_fn
        )
        self.exclude_diagonal = exclude_diagonal

        if self.group_sizes is not None:
            # Boolean mask for the main diagonal blocks corresponding to groups
            diag_blocks_mask = torch.block_diag(
                *[torch.ones((s, s), dtype=torch.bool) for s in self.group_sizes]
            )
            # Extract the upper triangular part
            self.register_buffer(
                "_upper_diag_blocks_mask",
                torch.triu(diag_blocks_mask, diagonal=int(self.exclude_diagonal)),
            )
        else:
            self._upper_diag_blocks_mask = None

    def forward(
        self,
        similarities_x: torch.Tensor,
        similarities_y: torch.Tensor,
        *,
        mats: Optional[Sequence[torch.Tensor]] = None,
    ) -> torch.Tensor:
        assert similarities_x.ndim >= 2 and similarities_y.ndim >= 2
        assert similarities_x.shape[-2:] == similarities_x.shape[-2:]

        if self._upper_diag_blocks_mask is None:
            mask = torch.triu(
                torch.ones(
                    similarities_x.shape[-2:],
                    dtype=torch.bool,
                    layout=similarities_x.layout,
                    device=similarities_x.device,
                ),
                diagonal=int(self.exclude_diagonal),
            )
        else:
            mask = self._upper_diag_blocks_mask

        scores = self.score_fn(similarities_x[..., mask], similarities_y[..., mask])
        loss = -scores

        return loss
