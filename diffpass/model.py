# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/model.ipynb.

# %% auto 0
__all__ = ['IndexPair', 'IndexPairsInGroup', 'IndexPairsInGroups', 'GeneralizedPermutation', 'MatrixApply',
           'PermutationConjugate', 'global_argmax_from_group_argmaxes', 'apply_hard_permutation_batch_to_similarity',
           'TwoBodyEntropyLoss', 'MILoss', 'HammingSimilarities', 'Blosum62Similarities', 'BestHits',
           'InterGroupSimilarityLoss', 'IntraGroupSimilarityLoss']

# %% ../nbs/model.ipynb 4
# Stdlib imports
from collections.abc import Iterable, Sequence
from typing import Optional, Union, Literal
from warnings import warn
from functools import partial

# NumPy
import numpy as np

# PyTorch
import torch
from torch.nn import Module, ParameterList, Parameter

# DiffPaSS imports
from .gumbel_sinkhorn_ops import gumbel_sinkhorn, gumbel_matching
from diffpass.entropy_ops import (
    smooth_mean_one_body_entropy,
    smooth_mean_two_body_entropy,
)
from .constants import get_blosum62_data
from diffpass.sequence_similarity_ops import (
    smooth_hamming_similarities_dot,
    smooth_hamming_similarities_cdist,
    smooth_substitution_matrix_similarities_dot,
    smooth_substitution_matrix_similarities_cdist,
    soft_best_hits,
    hard_best_hits,
)

# Type aliases
IndexPair = tuple[int, int]  # Pair of indices
IndexPairsInGroup = list[IndexPair]  # Pairs of indices in a group of sequences
IndexPairsInGroups = list[IndexPairsInGroup]  # Pairs of indices in groups of sequences

# %% ../nbs/model.ipynb 7
def _consecutive_slices_from_sizes(group_sizes: Optional[Sequence[int]]) -> list[slice]:
    if group_sizes is None:
        return [slice(None)]
    cumsum = np.cumsum(group_sizes).tolist()

    return [slice(start, end) for start, end in zip([0] + cumsum, cumsum)]

# %% ../nbs/model.ipynb 9
class GeneralizedPermutation(Module):
    """Generalized permutation layer implementing both soft and hard permutations."""

    def __init__(
        self,
        *,
        group_sizes: Sequence[int],
        batch_size: Optional[int] = None,
        fixed_pairings: Optional[IndexPairsInGroups] = None,
        tau: float = 1.0,
        n_iter: int = 1,
        noise: bool = False,
        noise_factor: float = 1.0,
        noise_std: bool = False,
        mode: Literal["soft", "hard"] = "soft",
    ) -> None:
        super().__init__()
        self.group_sizes = group_sizes
        self.batch_size = batch_size

        self.init_batch_size_fixed_pairings_and_log_alphas(batch_size, fixed_pairings)

        self.tau = tau
        self.n_iter = n_iter
        self.noise = noise
        self.noise_factor = noise_factor
        self.noise_std = noise_std
        self.mode = mode

    def init_batch_size_fixed_pairings_and_log_alphas(
        self,
        batch_size: Optional[int],
        fixed_pairings: Optional[IndexPairsInGroups],
        device: Optional[torch.device] = None,
    ) -> None:
        """Initialize fixed pairings and parameterization matrices."""
        self._validate_batch_size_and_fixed_pairings(batch_size, fixed_pairings)
        self.batch_size = batch_size
        self.fixed_pairings = fixed_pairings

        # Initialize parameterization matrices ('log-alphas')
        # By default, initialize all parametrization matrices to zero
        if self.batch_size is None:
            self.log_alphas = ParameterList(
                [
                    Parameter(torch.zeros(s, s), requires_grad=bool(s))
                    for s in self._effective_number_nonfixed_pairings
                ]
            )
        else:
            self.log_alphas = ParameterList(
                [
                    ParameterList(
                        [
                            Parameter(torch.zeros(s, s), requires_grad=bool(s))
                            for s in self._effective_number_nonfixed_pairings[batch_idx]
                        ]
                    )
                    for batch_idx in range(self.batch_size)
                ]
            )
        self.to(device=device)

    def _validate_fixed_pairings_single(
        self,
        fixed_pairings: Optional[IndexPairsInGroups] = None,
        batch_idx: Optional[int] = None,
    ) -> tuple[list[IndexPairsInGroup], list[int], list[int], int]:
        if fixed_pairings:
            if len(fixed_pairings) != len(self.group_sizes):
                raise ValueError(
                    "If `fixed_pairings` is provided, it must have the same length as "
                    "`group_sizes`."
                )
            for s, fp in zip(self.group_sizes, fixed_pairings):
                if not fp:
                    continue
                if any([len(p) != 2 for p in fp]):
                    raise ValueError(
                        "All fixed pairings must be pairs of indices (i, j)."
                    )
                if any(min(i, j) < 0 or max(i, j) >= s for i, j in fp):
                    raise ValueError(
                        "All fixed pairings must be within the range of the corresponding "
                        "group size."
                    )
            effective_number_fixed_pairings = []
            effective_number_nonfixed_pairings = []
            effective_fixed_pairings_zip = []
            for idx, (s, fp) in enumerate(zip(self.group_sizes, fixed_pairings)):
                if fp:
                    num_fp = len(fp)
                    fp_zip = list(zip(*fp))
                else:
                    num_fp = 0
                    fp_zip = [(), ()]
                complement = s - num_fp  # Effectively fully fixed when complement <= 1
                is_fully_fixed = complement <= 1
                num_efp = s - (s - num_fp) * (not is_fully_fixed)
                effective_number_fixed_pairings.append(num_efp)
                effective_number_nonfixed_pairings.append(s - num_efp)
                not_fixed_mask = getattr(self, f"_not_fixed_mask_{idx}")
                if is_fully_fixed:
                    not_fixed_mask[batch_idx, ...] = False
                    if complement:
                        possible_idxs = set(range(s))
                        fp_zip[0] += tuple((possible_idxs - set(fp_zip[0])))
                        fp_zip[1] += tuple((possible_idxs - set(fp_zip[1])))
                else:
                    for i, j in fp:
                        not_fixed_mask[batch_idx, j, :] = False
                        not_fixed_mask[batch_idx, :, i] = False
                effective_fixed_pairings_zip.append(fp_zip)
            total_number_fixed_pairings = sum(effective_number_fixed_pairings)
        else:
            effective_fixed_pairings_zip = [[(), ()] for _ in self.group_sizes]
            effective_number_fixed_pairings = [0] * len(self.group_sizes)
            effective_number_nonfixed_pairings = self.group_sizes
            total_number_fixed_pairings = 0

        return (
            effective_fixed_pairings_zip,
            effective_number_fixed_pairings,
            effective_number_nonfixed_pairings,
            total_number_fixed_pairings,
        )

    def _validate_batch_size_and_fixed_pairings(
        self,
        batch_size: Optional[int],
        fixed_pairings: Optional[
            Union[IndexPairsInGroups, list[IndexPairsInGroups]]
        ] = None,
    ) -> None:
        if batch_size is None:
            # No batching, expect fixed_pairings is Optional[IndexPairsInGroups] and use `_validate_fixed_pairings_single`
            for idx, s in enumerate(self.group_sizes):
                self.register_buffer(
                    f"_not_fixed_mask_{idx}", torch.ones(s, s, dtype=torch.bool)
                )
            (
                self._effective_fixed_pairings_zip,
                self._effective_number_fixed_pairings,
                self._effective_number_nonfixed_pairings,
                self._total_number_fixed_pairings,
            ) = self._validate_fixed_pairings_single(fixed_pairings)
        elif not isinstance(fixed_pairings, list) or len(fixed_pairings) != batch_size:
            raise ValueError(
                f"When `batch_size` is not None, `fixed_pairings` must be a list "
                f"with length equal to `batch_size`."
            )
        else:
            # Batching, expect fixed_pairings is list[IndexPairsInGroups]
            for idx, s in enumerate(self.group_sizes):
                self.register_buffer(
                    f"_not_fixed_mask_{idx}",
                    torch.ones(batch_size, s, s, dtype=torch.bool),
                )
            (
                self._effective_fixed_pairings_zip,
                self._effective_number_fixed_pairings,
                self._effective_number_nonfixed_pairings,
                self._total_number_fixed_pairings,
            ) = zip(
                *(
                    self._validate_fixed_pairings_single(
                        fixed_pairings_this_batch, batch_idx
                    )
                    for batch_idx, fixed_pairings_this_batch in enumerate(
                        fixed_pairings
                    )
                )
            )

    @property
    def _not_fixed_masks(self) -> list[torch.Tensor]:
        return [
            getattr(self, f"_not_fixed_mask_{idx}")
            for idx in range(len(self.group_sizes))
        ]

    @property
    def mode(self) -> str:
        return self._mode

    @mode.setter
    def mode(self, value) -> None:
        value = value.lower()
        if value not in ["soft", "hard"]:
            raise ValueError("mode must be either 'soft' or 'hard'.")
        self._mode = value.lower()
        _mat_fn_no_fixed = getattr(self, f"_{self._mode}_mat")
        if self.batch_size is None:
            self._fwd_one_group = (
                (lambda group_idx: _mat_fn_no_fixed(self.log_alphas[group_idx]))
                if not self.fixed_pairings
                else self._impl_fixed_pairings(_mat_fn_no_fixed)
            )
        else:
            self._fwd_one_group = self._impl_fixed_pairings_batched(_mat_fn_no_fixed)

    def soft_(self) -> None:
        self.mode = "soft"

    def hard_(self) -> None:
        self.mode = "hard"

    def _impl_fixed_pairings(self, func: callable) -> callable:
        """Include fixed pairings in the Gumbel-Sinkhorn or Gumbel-matching operators."""

        def wrapper(group_idx: int) -> torch.Tensor:
            s = self.group_sizes[group_idx]
            mat = func(self.log_alphas[group_idx])
            row_group, col_group = self._effective_fixed_pairings_zip[group_idx]
            mask = self._not_fixed_masks[group_idx]  # (s, s)
            mat_all = torch.zeros(
                s,
                s,
                dtype=mat.dtype,
                layout=mat.layout,
                device=mat.device,
            )
            # mat_all[j, i] = 1 means that row i becomes row j under a permutation,
            # using our conventions
            mat_all[..., col_group, row_group] = 1
            mat_all.masked_scatter_(mask, mat)

            return mat_all

        return wrapper

    def _impl_fixed_pairings_batched(self, func: callable) -> callable:
        """Include fixed pairings in the Gumbel-Sinkhorn or Gumbel-matching operators."""

        def wrapper(group_idx: int) -> torch.Tensor:
            s = self.group_sizes[group_idx]
            mats = [
                func(log_alphas_this_batch[group_idx])
                for log_alphas_this_batch in self.log_alphas
            ]
            row_groups, col_groups = zip(
                *[
                    effective_fixed_pairings_zip[group_idx]
                    for effective_fixed_pairings_zip in self._effective_fixed_pairings_zip
                ]
            )
            masks = self._not_fixed_masks[group_idx]  # (batch_size, s, s)
            mats_all = torch.zeros(
                self.batch_size,
                s,
                s,
                dtype=mats[0].dtype,
                layout=mats[0].layout,
                device=mats[0].device,
            )
            # mat_all[j, i] = 1 means that row i becomes row j under a permutation,
            # using our conventions
            for batch_idx, (row_group, col_group) in enumerate(
                zip(row_groups, col_groups)
            ):
                mats_all[batch_idx, col_group, row_group] = 1
                mats_all[batch_idx, ...].masked_scatter_(
                    masks[batch_idx], mats[batch_idx]
                )

            return mats_all

        return wrapper

    def _soft_mat(self, log_alpha: torch.Tensor) -> torch.Tensor:
        """Evaluate the Gumbel-Sinkhorn operator on `log_alpha`."""
        return gumbel_sinkhorn(
            log_alpha,
            tau=self.tau,
            n_iter=self.n_iter,
            noise=self.noise,
            noise_factor=self.noise_factor,
            noise_std=self.noise_std,
        )

    def _hard_mat(self, log_alpha: torch.Tensor) -> torch.Tensor:
        """Evaluate the Gumbel-matching operator on `log_alpha`."""
        return gumbel_matching(
            log_alpha,
            noise=self.noise,
            noise_factor=self.noise_factor,
            noise_std=self.noise_std,
            unbias_lsa=True,
        )

    def forward(self) -> list[torch.Tensor]:
        """Compute the soft/hard permutations, according to the current mode."""

        return [
            self._fwd_one_group(group_idx) for group_idx in range(len(self.group_sizes))
        ]


class MatrixApply(Module):
    """Apply matrices to chunks of a tensor of shape (n_samples, length, alphabet_size)
    and collate the results."""

    def __init__(self, group_sizes: Sequence[int]) -> None:
        super().__init__()
        self.group_sizes = group_sizes
        self._group_slices = _consecutive_slices_from_sizes(self.group_sizes)

    def forward(self, x: torch.Tensor, *, mats: Sequence[torch.Tensor]) -> torch.Tensor:
        ensemble_shape = mats[0].shape[:-2]
        out = torch.full_like(x, torch.nan).repeat(*ensemble_shape, 1, 1, 1)
        for mats_this_group, sl in zip(mats, self._group_slices):
            out[..., sl, :, :].copy_(
                torch.tensordot(mats_this_group, x[sl, :, :], dims=1)
            )

        return out


class PermutationConjugate(Module):
    """Conjugate blocks of a square 2D tensor of shape (n_samples, n_samples) by
    permutation matrices."""

    def __init__(self, group_sizes: Sequence[int]) -> None:
        super().__init__()
        self.group_sizes = group_sizes
        self._group_slices = _consecutive_slices_from_sizes(self.group_sizes)

    def forward(self, x: torch.Tensor, *, mats: Sequence[torch.Tensor]) -> torch.Tensor:
        ensemble_shape = mats[0].shape[:-2]
        out1 = torch.full_like(x, torch.nan).repeat(*ensemble_shape, 1, 1)
        out2 = torch.full_like(x, torch.nan).repeat(*ensemble_shape, 1, 1)
        # (P * A) * P.T
        for mats_this_group, sl in zip(mats, self._group_slices):
            out1[..., sl, :].copy_(torch.tensordot(mats_this_group, x[sl, :], dims=1))
        for mats_this_group, sl in zip(mats, self._group_slices):
            out2[..., :, sl].copy_(
                torch.einsum("...ij,...kj->...ik", out1[..., :, sl], mats_this_group)
            )

        return out2


def global_argmax_from_group_argmaxes(mats: Iterable[torch.Tensor]) -> torch.Tensor:
    global_argmax = []
    start_idx = 0
    for mats_this_group in mats:
        global_argmax.append(mats_this_group.argmax(-1) + start_idx)
        start_idx += mats_this_group.shape[-1]

    return torch.cat(global_argmax, dim=-1)


def apply_hard_permutation_batch_to_similarity(
    *, x: torch.Tensor, perms: list[torch.Tensor]
) -> torch.Tensor:
    """
    Conjugate a single square matrix by a batch of hard permutations.

    Args:
        perms: List of batches of permutation matrices of shape (..., D, D).
        x: Matrix of shape (D, D).

    Returns:
        Batch of conjugated matrices of shape (..., D, D).
    """
    global_argmax = global_argmax_from_group_argmaxes(perms)
    x_permuted_rows = x[global_argmax]

    # Permuting columns is more involved
    index = global_argmax.view(*global_argmax.shape[:-1], 1, -1).expand(
        *global_argmax.shape, global_argmax.shape[-1]
    )
    # Example of gather with 4D tensor and dim=-1:
    # out[i][j][k][l] = input[i][j][k][index[i][j][k][l]]

    return torch.gather(x_permuted_rows, -1, index)

# %% ../nbs/model.ipynb 13
class TwoBodyEntropyLoss(Module):
    """Differentiable extension of the mean of estimated two-body entropies between
    all pairs of columns from two one-hot encoded tensors."""

    def __init__(self):
        super().__init__()

    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        return smooth_mean_two_body_entropy(x, y)


class MILoss(Module):
    """Differentiable extension of minus the mean of estimated mutual informations
    between all pairs of columns from two one-hot encoded tensors."""

    def __init__(self):
        super().__init__()

    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        return smooth_mean_two_body_entropy(x, y) - smooth_mean_one_body_entropy(x)

# %% ../nbs/model.ipynb 18
class HammingSimilarities(Module):
    """Compute Hamming similarities between sequences using differentiable
    operations.

    Optionally, if the sequences are arranged in groups, the computation of
    similarities can be restricted to within groups.
    Differentiable operations are used to compute the similarities, which can be
    either dot products or an L^p distance function."""

    def __init__(
        self,
        *,
        group_sizes: Optional[Sequence[int]] = None,
        use_dot: bool = True,
        p: Optional[float] = None,
    ) -> None:
        super().__init__()
        self.group_sizes = group_sizes
        self.use_dot = use_dot
        self.p = p

        if self.use_dot:
            if self.p is not None:
                warn("Since a `p` was provided, `use_dot` will be ignored.")
            self._similarities_fn = smooth_hamming_similarities_dot
            self._similarities_fn_kwargs = {}
        else:
            if self.p is None:
                raise ValueError("If `use_dot` is False, `p` must be provided.")
            self._similarities_fn = smooth_hamming_similarities_cdist
            self._similarities_fn_kwargs = {"p": self.p}

        self._group_slices = _consecutive_slices_from_sizes(self.group_sizes)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        size = x.shape[:-3] + (x.shape[-3],) * 2
        out = torch.full(
            size, torch.nan, dtype=x.dtype, layout=x.layout, device=x.device
        )
        for sl in self._group_slices:
            out[..., sl, sl].copy_(
                self._similarities_fn(x[..., sl, :, :], **self._similarities_fn_kwargs)
            )

        return out


class Blosum62Similarities(Module):
    """Compute Blosum62-based similarities between sequences using differentiable
    operations.

    Optionally, if the sequences are arranged in groups, the computation of
    similarities can be restricted to within groups.
    Differentiable operations are used to compute the similarities, which can be
    either dot products or an L^p distance function."""

    def __init__(
        self,
        *,
        group_sizes: Optional[Sequence[int]] = None,
        use_dot: bool = True,
        p: Optional[float] = None,
        use_scoredist: bool = False,
        aa_to_int: Optional[dict[str, int]] = None,
        gaps_as_stars: bool = True,
    ) -> None:
        super().__init__()
        self.group_sizes = group_sizes
        self.use_dot = use_dot
        self.p = p
        self.use_scoredist = use_scoredist
        self.aa_to_int = aa_to_int
        self.gaps_as_stars = gaps_as_stars

        blosum62_data = get_blosum62_data(
            aa_to_int=self.aa_to_int, gaps_as_stars=self.gaps_as_stars
        )
        self.register_buffer("subs_mat", blosum62_data.mat)
        self.expected_value = blosum62_data.expected_value

        self._similarities_fn_kwargs = {"subs_mat": self.subs_mat}
        if self.use_dot:
            if self.p is not None:
                warn("Since a `p` was provided, `use_dot` will be ignored.")
            self._similarities_fn = smooth_substitution_matrix_similarities_dot
            self._similarities_fn_kwargs = {
                "use_scoredist": self.use_scoredist,
                "expected_value": self.expected_value,
            }
        else:
            if self.p is None:
                raise ValueError("If `use_dot` is False, `p` must be provided.")
            self._similarities_fn = smooth_substitution_matrix_similarities_cdist
            self._similarities_fn_kwargs = {"p": self.p}

        self._group_slices = _consecutive_slices_from_sizes(self.group_sizes)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        size = x.shape[:-3] + (x.shape[-3],) * 2
        out = torch.full(
            size, torch.nan, dtype=x.dtype, layout=x.layout, device=x.device
        )
        for sl in self._group_slices:
            out[..., sl, sl].copy_(
                self._similarities_fn(
                    x[..., sl, :, :],
                    subs_mat=self.subs_mat,
                    **self._similarities_fn_kwargs,
                )
            )

        return out

# %% ../nbs/model.ipynb 23
class BestHits(Module):
    """Compute (reciprocal) best hits within and between groups of sequences,
    starting from a similarity matrix.

    Best hits can be either 'hard', in which cases they are computed using the
    argmax, or 'soft', in which case they are computed using the softmax with a
    temperature parameter `tau`. In both cases, the main diagonal in the similarity
    matrix is excluded by setting its entries to minus infinity."""

    def __init__(
        self,
        *,
        reciprocal: bool = True,
        group_sizes: Optional[Sequence[int]],
        tau: float = 0.1,
        mode: Literal["soft", "hard"] = "soft",
    ) -> None:
        super().__init__()
        self.reciprocal = reciprocal
        self.group_sizes = group_sizes
        self._group_slices = _consecutive_slices_from_sizes(self.group_sizes)
        self.tau = tau
        self.mode = mode

    @property
    def mode(self) -> str:
        return self._mode

    @mode.setter
    def mode(self, value) -> None:
        value = value.lower()
        if value not in ["soft", "hard"]:
            raise ValueError("`mode` must be either 'soft' or 'hard'.")
        self._mode = value.lower()
        self._bh_fn = getattr(self, f"_{self._mode}_bh_fn")

    def soft_(self) -> None:
        self.mode = "soft"

    def hard_(self) -> None:
        self.mode = "hard"

    def _soft_bh_fn(self, similarities: torch.Tensor) -> torch.Tensor:
        """Compute soft best hits."""
        return soft_best_hits(
            similarities,
            reciprocal=self.reciprocal,
            group_slices=self._group_slices,
            tau=self.tau,
        )

    def _hard_bh_fn(self, similarities: torch.Tensor) -> torch.Tensor:
        """Compute hard best hits."""
        return hard_best_hits(
            similarities,
            reciprocal=self.reciprocal,
            group_slices=self._group_slices,
        )

    def forward(self, similarities: torch.Tensor) -> torch.Tensor:
        return self._bh_fn(similarities)

# %% ../nbs/model.ipynb 26
class InterGroupSimilarityLoss(Module):
    """Compute a loss that compares similarity matrices restricted to inter-group
    relationships.

    Similarity matrices are expected to be square and symmetric. The loss is computed
    by comparing the (flattened and concatenated) blocks containing inter-group
    similarities."""

    def __init__(
        self,
        *,
        # Number of entries in each group (e.g. species). Groups are assumed to be
        # contiguous in the input similarity matrices
        group_sizes: Sequence[int],
        # If not ``None``, custom callable to compute the differentiable score between
        # the flattened and concatenated inter-group blocks of the similarity matrices.
        # Default: dot product
        score_fn: Union[callable, None] = None,
    ) -> None:
        super().__init__()
        self.group_sizes = group_sizes
        self.score_fn = (
            partial(torch.tensordot, dims=1) if score_fn is None else score_fn
        )

        diag_blocks_mask = torch.block_diag(
            *[torch.ones((s, s), dtype=torch.bool) for s in self.group_sizes]
        )
        self.register_buffer(
            "_upper_no_diag_blocks_mask", torch.triu(~diag_blocks_mask)
        )

    def forward(
        self,
        similarities_x: torch.Tensor,
        similarities_y: torch.Tensor,
        *,
        mats: Optional[Sequence[torch.Tensor]] = None,
    ) -> torch.Tensor:
        # Input validation
        assert similarities_x.ndim >= 2 and similarities_y.ndim >= 2

        scores = self.score_fn(
            similarities_x[..., self._upper_no_diag_blocks_mask],
            similarities_y[..., self._upper_no_diag_blocks_mask],
        )
        loss = -scores

        return loss


class IntraGroupSimilarityLoss(Module):
    """Compute a loss that compares similarity matrices restricted to intra-group
    relationships.

    Similarity matrices are expected to be square and symmetric. Their diagonal
    elements are ignored if `exclude_diagonal` is set to ``True``.
    If `group_sizes` is provided, the loss is computed by comparing the flattened
    and concatenated upper triangular blocks containing intra-group similarities.
    Otherwise, the loss is computed by comparing the upper triangular part of the
    full similarity matrices."""

    def __init__(
        self,
        *,
        # Number of entries in each group (e.g. species). Groups are assumed to be
        # contiguous in the input similarity matrices
        group_sizes: Optional[Sequence[int]] = None,
        # If not ``None``, custom callable to compute the differentiable score between
        # the flattened and concatenated intra-group blocks of the similarity matrices
        # Default: dot product
        score_fn: Union[callable, None] = None,
        # If ``True``, exclude the diagonal elements from the computation
        exclude_diagonal: bool = True,
    ) -> None:
        super().__init__()
        self.group_sizes = group_sizes
        self.score_fn = (
            partial(torch.tensordot, dims=1) if score_fn is None else score_fn
        )
        self.exclude_diagonal = exclude_diagonal

        if self.group_sizes is not None:
            # Boolean mask for the main diagonal blocks corresponding to groups
            diag_blocks_mask = torch.block_diag(
                *[torch.ones((s, s), dtype=torch.bool) for s in self.group_sizes]
            )
            # Extract the upper triangular part
            self.register_buffer(
                "_upper_diag_blocks_mask",
                torch.triu(diag_blocks_mask, diagonal=int(self.exclude_diagonal)),
            )
        else:
            self._upper_diag_blocks_mask = None

    def forward(
        self,
        similarities_x: torch.Tensor,
        similarities_y: torch.Tensor,
        *,
        mats: Optional[Sequence[torch.Tensor]] = None,
    ) -> torch.Tensor:
        assert similarities_x.ndim >= 2 and similarities_y.ndim >= 2
        assert similarities_x.shape[-2:] == similarities_x.shape[-2:]

        if self._upper_diag_blocks_mask is None:
            mask = torch.triu(
                torch.ones(
                    similarities_x.shape[-2:],
                    dtype=torch.bool,
                    layout=similarities_x.layout,
                    device=similarities_x.device,
                ),
                diagonal=int(self.exclude_diagonal),
            )
        else:
            mask = self._upper_diag_blocks_mask

        scores = self.score_fn(similarities_x[..., mask], similarities_y[..., mask])
        loss = -scores

        return loss
